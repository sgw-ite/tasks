{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Feature_Engineering\n",
    "import metrics\n",
    "import train_test_split as tts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './train.csv'\n",
    "X_train, y_train = Feature_Engineering.datatransform(file)\n",
    "X_train, y_train, X_test, y_test = tts.holdout(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = y_train.reshape((-1,1))\n",
    "Y_test = y_test.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(622, 14) (267, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    \n",
    "    '''\n",
    "    层。\n",
    "    储存W,b,A,Z矩阵。\n",
    "    提供forward（激活），backward（计算在输入值处的激活函数的导数）接口。\n",
    "    -----\n",
    "    params:\n",
    "    Z : 输入值组成的矩阵。shape=(样本个数，本层节点个数)\n",
    "    A : 激活值组成的矩阵。shape同Z\n",
    "    B : 偏置单元\n",
    "    dZ : 残差用于更新W\n",
    "    dgZ : 激活函数在z点的导数值\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, method='test', nodenum=0, nextnodenum=0, size=0):\n",
    "        \n",
    "        self.method = method\n",
    "        self.nodenum = nodenum\n",
    "        self.nextnodenum = nextnodenum\n",
    "        self.size = size\n",
    "        self.W = np.ones((self.nodenum,self.nextnodenum))\n",
    "        for i in range(self.nodenum):\n",
    "            for j in range(self.nextnodenum):\n",
    "                self.W[i][j] = np.random.random(1) ############### 直接用 np.random.random((self.nodenum, self.nextnodenum))\n",
    "        #print(self.nextnodenum)\n",
    "        self.b = np.ones((1,self.nextnodenum))\n",
    "        for i in range(self.nextnodenum):\n",
    "            self.b[0][i] = np.random.random(1)\n",
    "        self.Z = np.ones((self.size,self.nodenum))\n",
    "        for i in range(size):\n",
    "            for j in range(self.nodenum):\n",
    "                self.Z[i][j] = np.random.random(1)\n",
    "        self.A = self.Z.copy()\n",
    "        self.dZ = self.Z.copy()\n",
    "        self.dgZ = self.Z.copy()\n",
    "        self.B = np.ones((size,1))\n",
    "        \n",
    "    def _ReLU(self, Z, direction):\n",
    "        if direction=='forward':\n",
    "            for i in range(Z.shape[0]): ############### np.clip(Z, 0)\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = 0\n",
    "            return Z\n",
    "        for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = 0\n",
    "                    else: \n",
    "                        Z[i][j] = 1\n",
    "        return Z\n",
    "    \n",
    "    def _sigmoid(self, z, direction):\n",
    "        haha = (1 / (1 + np.exp(-z)))\n",
    "        if direction=='forward':\n",
    "            return haha\n",
    "        return haha*(1-haha)\n",
    "    \n",
    "    def _tanh(self, z, direction):\n",
    "        if direction=='forward':\n",
    "            return ((np.exp(2*z) - 1) / (np.exp(2*z) +1))\n",
    "        return (4 * (np.exp(2*z) / ((np.exp(2*z) + 1)**2)))\n",
    "    \n",
    "    def _test(self, z, direction):\n",
    "        if direction=='forward':\n",
    "            return z\n",
    "        return 1\n",
    "    \n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        if self.method=='ReLU':\n",
    "            self.A = self._ReLU(self.Z, direction='forward')\n",
    "        elif self.method=='sigmoid':\n",
    "            self.A = self._sigmoid(self.Z, direction='forward')\n",
    "        elif self.method=='tanh':\n",
    "            self.A = self._tanh(self.Z, direction='forward')\n",
    "        elif self.method=='test':\n",
    "            self.A = self._test(self.Z, direction='forward')\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        if self.method=='ReLU':\n",
    "            self.dgZ = self._ReLU(self.Z, direction='backward')\n",
    "        elif self.method=='sigmoid':\n",
    "            self.dgZ = self._sigmoid(self.Z, direction='backward')\n",
    "        elif self.method=='tanh':\n",
    "            self.dgZ = self._tanh(self.Z, direction='backward')\n",
    "        elif self.method=='test':\n",
    "            self.dgZ = self._test(self.Z, direction='backward')\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork():\n",
    "    \n",
    "    def __init__(self, X_train, Y_train, layer_num=3, eta=0.5, lamda=0.1):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.layer_num = layer_num\n",
    "        self.eta = eta\n",
    "        self.lamda = lamda\n",
    "        #构建网络\n",
    "        self.size = self.X_train.shape[0]\n",
    "        self.layers = []\n",
    "        #输入层：\n",
    "        inlayer = layer(nodenum=X_train.shape[1], nextnodenum=20, size=self.size)\n",
    "        self.layers.append(inlayer)\n",
    "        self.__iter = 0\n",
    "        #隐藏层：\n",
    "        if self.layer_num==3:\n",
    "            hidlayer = layer(nodenum=20, nextnodenum=self.Y_train.shape[1], size=self.size)\n",
    "            ####################### 这里多了一个输出层，注释了就好了，你看看到底是哪里写错了吧（\n",
    "            #outlayer = layer(nodenum=self.Y_train.shape[1], nextnodenum=self.Y_train.shape[1], size=self.size, method='sigmoid')#nextnodenum无意义\n",
    "            self.layers.append(hidlayer)\n",
    "            #self.layers.append(outlayer)\n",
    "        else:\n",
    "            for i in range(self.layer_num-3):\n",
    "                hidlayer = layer(nodenum=20, nextnodenum=20, size=self.size)\n",
    "                self.layers.append(hidlayer)\n",
    "            hidlayer = layer(nodenum=20, nextnodenum=self.Y_train.shape[1], size=self.size)\n",
    "        #输出层：\n",
    "        outlayer = layer(nodenum=self.Y_train.shape[1], nextnodenum=2, size=self.size, method='sigmoid') #nextnodenum无意义\n",
    "        self.layers.append(outlayer)\n",
    "        #网络构建完毕。\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        #对第一层：\n",
    "        self.layers[0].A = self.X_train.copy()\n",
    "        \n",
    "        #接下来的每一层：\n",
    "        for i in range(self.layer_num-1):\n",
    "            self.layers[i+1].Z = self.layers[i].A.dot(self.layers[i].W) + self.layers[i].B.dot(self.layers[i].b)\n",
    "            self.layers[i+1].forward()\n",
    "        #print('3',self.layers[num-2-i].W)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        # 从输出层开始：\n",
    "        self.layers[-1].backward()\n",
    "        self.layers[-1].dZ = (self.layers[-1].A - self.Y_train) * (self.layers[-1].dgZ)\n",
    "        \n",
    "        # 依次往后传播,计算dz：\n",
    "        num = self.layer_num #为了下面的代码简洁一点\n",
    "        for i in range(num-1):\n",
    "            self.layers[num-2-i].backward()\n",
    "            self.layers[num-2-i].dZ = self.layers[num-1-i].dZ.dot(self.layers[num-2-i].W.T) * self.layers[num-2-i].dgZ\n",
    "            \n",
    "        # 更新W,b：\n",
    "        \n",
    "        for i in range(num-1):\n",
    "            dW = (self.layers[num-2-i].W * self.lamda) + ((self.layers[num-2-i].A.T.dot(self.layers[num-1-i].dZ)) / self.size)\n",
    "            db = np.sum((self.layers[num-1-i].dZ / self.size), axis=0).reshape(1,-1)\n",
    "            if self.__iter%10==0:\n",
    "                print('layer :',i,'\\ndW:',dW[:3,:3],'\\ndb:',db[:3],'\\n',self.__iter)\n",
    "            #print('1',self.layers[num-2-i].W)\n",
    "            self.layers[num-2-i].W = self.layers[num-2-i].W - (self.eta * dW)\n",
    "            #print('2',self.layers[num-2-i].b)\n",
    "            self.layers[num-2-i].b = self.layers[num-2-i].b - (self.eta * db)\n",
    "            \n",
    "        self.__iter += 1\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier():\n",
    "    \n",
    "    def __init__(self, network, X_train, Y_train, eta=0.5, lamda=0.1, epsilon=1e-4):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "        self.lamda = lamda\n",
    "        self.mynetwork = neuralnetwork(self.X_train, self.Y_train)\n",
    "        \n",
    "    def _J(self, network, Y_train):\n",
    "        y_pred = network.layers[-1].A\n",
    "        W2 = 0\n",
    "        for i in range(network.layer_num):\n",
    "            W2 += np.sum(network.layers[i].W ** 2)\n",
    "        J = 0.5 * np.sum((Y_train - y_pred)**2) + self.lamda * 0.5 * W2\n",
    "        return J\n",
    "        \n",
    "    def fit(self):\n",
    "        self.mynetwork.forward()\n",
    "        _iter = 0\n",
    "        #y_pred = mynetwork.layers[-1].A\n",
    "        while(_iter<1000):\n",
    "            J1 = self._J(Y_train=self.Y_train, network=self.mynetwork)\n",
    "            self.mynetwork.backward()\n",
    "            self.mynetwork.forward()\n",
    "            J2 = self._J(Y_train=self.Y_train, network=self.mynetwork)\n",
    "            #print('4',self.mynetwork.layers[0].W, mynetwork2.layers[0].W)\n",
    "            print(J1)\n",
    "            delta = J1 -J2\n",
    "            \n",
    "            if (delta) < self.epsilon: ########### 之前给delta加了abs，不应该加\n",
    "                break\n",
    "            \n",
    "            _iter += 1\n",
    "            if _iter%50==0:\n",
    "                print('\\ndelta=',delta)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test, Y_test):\n",
    "        self.newnetwork = neuralnetwork(X_train=X_test, Y_train=Y_test)\n",
    "        for i in range(self.newnetwork.layer_num):\n",
    "            self.newnetwork.layers[i].W = self.mynetwork.layers[i].W.copy()\n",
    "            self.newnetwork.layers[i].b = self.mynetwork.layers[i].b.copy()\n",
    "        self.newnetwork.forward()\n",
    "        y_pred = self.newnetwork.layers[-1].A\n",
    "        return np.array(y_pred>0.5, dtype='int')\n",
    "    \n",
    "    def score(self, X_test, Y_test, scoring=metrics.acc_score):\n",
    "        y_pred = self.predict(X_test,Y_test).reshape(-1,).astype('float64')\n",
    "        y_test = Y_test.reshape(-1,)\n",
    "        print(y_pred, '1', y_test)\n",
    "        return scoring(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer : 0 \n",
      "dW: [[0.18528443]\n",
      " [0.44604608]\n",
      " [0.52462823]] \n",
      "db: [[0.49347562]] \n",
      " 0\n",
      "layer : 1 \n",
      "dW: [[0.01758434 0.03583579 0.08325871]\n",
      " [0.04255193 0.0545159  0.09620597]\n",
      " [0.08775968 0.01951297 0.06747349]] \n",
      "db: [[0.49013487 0.33655189 0.39114208 0.19549248 0.33358459 0.05118324\n",
      "  0.26331811 0.4111041  0.44606657 0.30391553 0.20364392 0.36514409\n",
      "  0.30937759 0.03305553 0.42665405 0.23630249 0.22648348 0.01729648\n",
      "  0.04200525 0.24726487]] \n",
      " 0\n",
      "109.92205514379951\n",
      "109.30387911592184\n",
      "108.80677073375267\n",
      "108.40000809216534\n",
      "108.07037584225431\n",
      "107.81617947519955\n",
      "107.64525246434843\n",
      "107.57561543868508\n",
      "CPU times: user 61.2 ms, sys: 3.85 ms, total: 65 ms\n",
      "Wall time: 59.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nnc = NeuralNetworkClassifier(network=neuralnetwork, X_train=X_train, Y_train=Y_train)\n",
    "nnc.fit() # 最好也是能把loss的下降可视化出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48874598]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array(nnc.mynetwork.layers[-1].A > 0.5, dtype='int')\n",
    "print(sum(y_pred==Y_train)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0.] 1 [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1.]\n",
      "0.5580524344569289\n"
     ]
    }
   ],
   "source": [
    "print(nnc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
