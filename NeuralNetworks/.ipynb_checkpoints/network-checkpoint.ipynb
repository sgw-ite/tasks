{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Feature_Engineering\n",
    "import metrics\n",
    "import train_test_split as tts\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    \n",
    "    '''\n",
    "    层。\n",
    "    储存W,b,A,Z矩阵。\n",
    "    提供forward（激活），backward（计算在输入值处的激活函数的导数）接口。\n",
    "    -----\n",
    "    params:\n",
    "    Z : 输入值组成的矩阵。shape=(样本个数，本层节点个数)\n",
    "    A : 激活值组成的矩阵。shape同Z\n",
    "    B : 偏置单元\n",
    "    dZ : 残差用于更新W\n",
    "    dgZ : 激活函数在z点的导数值\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, method='tanh', nodenum=1, lastnodenum=1, size=1):\n",
    "        \n",
    "        self.method = method\n",
    "        self.nodenum = nodenum\n",
    "        self.lastnodenum = lastnodenum\n",
    "        self.size = size\n",
    "        self.W = (np.vstack([np.random.randn(self.lastnodenum,self.nodenum) / \n",
    "                             np.sqrt(self.lastnodenum*self.nodenum),\n",
    "                             np.zeros((1,self.nodenum))]))\n",
    "        #这里采用cs231n中提到的对ReLU效果很好的一种方式：除以总数的平方根以使得初始的权重方差为1，并且将偏置项权重合并进去，初始化为零\n",
    "        self.Z = np.ones((self.size,self.nodenum))\n",
    "        self.A = np.hstack([self.Z.copy(),np.ones((self.size,1))]) #加入了偏置单元\n",
    "        self.dZ = self.Z.copy()\n",
    "        self.dgZ = self.A.copy()\n",
    "        \n",
    "    def _ReLU(self, Z, direction):\n",
    "        if direction=='forward':\n",
    "            for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = 0\n",
    "            return Z\n",
    "        for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = 0\n",
    "                    else: \n",
    "                        Z[i][j] = 1\n",
    "        return Z\n",
    "    \n",
    "    def _LeakyReLU(self, Z, direction):\n",
    "        if direction=='forward':\n",
    "            for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] *= 0.01\n",
    "            return Z\n",
    "        for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = 0.01\n",
    "                    else: \n",
    "                        Z[i][j] = 1\n",
    "        return Z\n",
    "    \n",
    "    def _maxout(self, Z, direction, l_num=2):\n",
    "        \n",
    "        if direction=='forward':\n",
    "            for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = w1*Z[i][j] + b1\n",
    "                    else: \n",
    "                        Z[i][j] = w2*Z[i][j] + b2\n",
    "            return Z\n",
    "        for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    if Z[i][j]<=0:\n",
    "                        Z[i][j] = w1\n",
    "                    else: \n",
    "                        Z[i][j] = w2\n",
    "        return Z\n",
    "    \n",
    "    def _sigmoid(self, z, direction):\n",
    "        haha = (1 / (1 + np.exp(-z)))\n",
    "        if direction=='forward':\n",
    "            return haha\n",
    "        return haha*(1-haha)\n",
    "    \n",
    "    def _tanh(self, z, direction):\n",
    "        if direction=='forward':\n",
    "            return ((np.exp(2*z) - 1) / (np.exp(2*z) +1))\n",
    "        return (4 * (np.exp(2*z) / ((np.exp(2*z) + 1)**2)))\n",
    "    \n",
    "    def _test(self, z, direcction):\n",
    "        if direction=='forward':\n",
    "            return z\n",
    "        return 1\n",
    "    \n",
    "    def _softmax(self, Z, direction):\n",
    "        if direction=='forward':\n",
    "            for z in Z:\n",
    "                z -= np.max(z)\n",
    "                z = z / mp.sum(z)\n",
    "            return Z\n",
    "        pass #softmax层不用求本地梯度\n",
    "    \n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        if self.method=='ReLU':\n",
    "            self.A = np.hstack([self._ReLU(self.Z, direction='forward'),np.ones((self.size,1))])\n",
    "        elif self.method=='sigmoid':\n",
    "            self.A = np.hstack([self._sigmoid(self.Z, direction='forward'),np.ones((self.size,1))])\n",
    "        elif self.method=='LeakyReLU':\n",
    "            self.A = np.hstack([self._LeakyReLU(self.Z, direction='forward'),np.ones((self.size,1))])\n",
    "        elif self.method=='maxout':\n",
    "            self.A = np.hstack([self._maxout(self.Z, direction='forward'),np.ones((self.size,1))])\n",
    "        elif self.method=='tanh':\n",
    "            self.A = np.hstack([self._tanh(self.Z, direction='forward'),np.ones((self.size,1))])\n",
    "        elif self.method=='test':\n",
    "            self.A = np.hstack([self._test(self.Z, direction='forward'),np.ones((self.size,1))])\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        if self.method=='ReLU':\n",
    "            self.dgZ = self._ReLU(self.Z, direction='backward')\n",
    "        elif self.method=='LeakyReLU':\n",
    "            self.A = self._LeakyReLU(self.Z, direction='backward')\n",
    "        elif self.method=='maxout':\n",
    "            self.A = self._maxout(self.Z, direction='backward')\n",
    "        elif self.method=='sigmoid':\n",
    "            self.dgZ = self._sigmoid(self.Z, direction='backward')\n",
    "        elif self.method=='tanh':\n",
    "            self.dgZ = self._tanh(self.Z, direction='backward')\n",
    "        elif self.method=='test':\n",
    "            self.dgZ = self._test(self.Z, direction='backward')\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork():\n",
    "    \n",
    "    def __init__(self, X_train, Y_train, layer_num=3, eta=0.5, lamda=0.1, hidlayer_nodenum=10, method='softmax'):\n",
    "        self.method = method\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.layer_num = layer_num\n",
    "        self.hidlayer_nodenum = hidlayer_nodenum\n",
    "        self.eta = eta\n",
    "        self.lamda = lamda\n",
    "        #构建网络\n",
    "        self.size = self.X_train.shape[0]\n",
    "        self.layers = []\n",
    "        #输入层：\n",
    "        inlayer = layer(nodenum=X_train.shape[1], size=self.size)\n",
    "        self.layers.append(inlayer)\n",
    "        self.__iter = 0\n",
    "        #隐藏层和输出层：\n",
    "        if self.layer_num>=3:\n",
    "            hidlayer = layer(lastnodenum=self.X_train.shape[1], nodenum=self.hidlayer_nodenum, size=self.size)\n",
    "            self.layers.append(hidlayer)\n",
    "            if self.layer_num>3:\n",
    "                for i in range(self.layer_num-3):\n",
    "                    print('???')\n",
    "                    hidlayer = layer(nodenum=self.hidlayer_nodenum, lastnodenum=self.hidlayer_nodenum, size=self.size)\n",
    "                    self.layers.append(hidlayer)\n",
    "            outlayer = layer(nodenum=self.Y_train.shape[1], lastnodenum=self.hidlayer_nodenum, \n",
    "                            size=self.size, method='sigmoid')\n",
    "            self.layers.append(outlayer)\n",
    "            print(self.layers)\n",
    "        else: #即layernum=2时\n",
    "            outlayer = layer(nodenum=self.Y_train.shape[1], lastnodenum=self.X_train.shape[1], \n",
    "                             size=self.size, method='sigmoid')\n",
    "            self.layers.append(outlayer)\n",
    "        #网络构建完毕。\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        #对第一层：\n",
    "        self.layers[0].A = np.hstack((self.X_train.copy(),np.ones((self.size,1))))\n",
    "                                     \n",
    "        \n",
    "        #接下来的每一层：\n",
    "        for i in range(1, self.layer_num):\n",
    "            self.layers[i].Z = self.layers[i-1].A.dot(self.layers[i].W)\n",
    "            self.layers[i].forward()\n",
    "        #print('3',self.layers[num-2-i].W)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        # 从输出层开始：\n",
    "        if self.method=='sigmoid':\n",
    "            self.layers[-1].backward()\n",
    "            self.layers[-1].dZ = ((self.layers[-1].A[:,:(self.layers[-1].A.shape[1]-1)] - self.Y_train) * \n",
    "                                  (self.layers[-1].dgZ))\n",
    "        if self.method=='softmax':\n",
    "            for i in range(self.layers[-1].dZ.shape[0]):\n",
    "                self.layers[-1].dZ[i][self.Y_train[i]==1] = (self.layers[-1].A[i,:(self.layers[-1].A.shape[1]-1)])[self.Y_train[i]==1] - 1\n",
    "                self.layers[-1].dZ[i][self.Y_train[i]==0] = (self.layers[-1].A[i,:(self.layers[-1].A.shape[1]-1)])[self.Y_train[i]==0]\n",
    "        \n",
    "        # 依次往后传播,计算dz：\n",
    "        num = self.layer_num #为了下面的代码简洁一点\n",
    "        for i in range(num-1):\n",
    "            self.layers[num-2-i].backward()\n",
    "            self.layers[num-2-i].dZ=(self.layers[num-1-i].dZ.dot(self.layers[num-1-i].W[:(self.layers[num-1-i].W.shape[0]-1),:].T)\n",
    "                                     * self.layers[num-2-i].dgZ)\n",
    "            \n",
    "        # 更新W,b：\n",
    "        \n",
    "        for i in range(1,num):\n",
    "            dW = (self.layers[i].W * self.lamda) + ((self.layers[i-1].A.T.dot(self.layers[i].dZ)) / self.size)\n",
    "            #if self.__iter%10==0:\n",
    "                #print('layer :',i,'\\ndW:',dW[:3,:3])\n",
    "            #print('1',self.layers[num-2-i].W)\n",
    "            self.layers[i].W = self.layers[i].W - (self.eta * dW)\n",
    "            #print('2',self.layers[num-2-i].b)\n",
    "            #self.layers[num-2-i].b = self.layers[num-2-i].b - (self.eta * db)\n",
    "            \n",
    "        self.__iter += 1\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier():\n",
    "    \n",
    "    def __init__(self, network, X_train, Y_train, eta=0.005, lamda=0.01, epsilon=1e-4, layer_num=3, hidlayer_nodenum, method='softmax'):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.layer_num = layer_num\n",
    "        self.hidlayer_nodenum = hidlayer_nodenum\n",
    "        self.method = method\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "        self.lamda = lamda\n",
    "        self.mynetwork = neuralnetwork(self.X_train, self.Y_train, layer_num=layer_num, lamda=self.lamda)\n",
    "        \n",
    "    def _J(self, network, Y_train):\n",
    "        y_pred = network.layers[-1].A[:,:(network.layers[-1].A.shape[1]-1)]\n",
    "        W2 = 0\n",
    "        J = 0\n",
    "        for i in range(network.layer_num):\n",
    "            W2 += np.sum(network.layers[i].W ** 2)\n",
    "        if self.method=='sigmoid':    \n",
    "            J = 0.5 * np.sum((Y_train - y_pred)**2) + self.lamda * 0.5 * W2\n",
    "        if self.method=='softmax':\n",
    "            for i in range(y_pred.shape[0]):\n",
    "                assert y_pred[i].size >0\n",
    "                J += -np.log((y_pred[i][self.Y_train[i]==1] / np.sum(y_pred[i])) + 1e-7)\n",
    "        return J\n",
    "        \n",
    "    def fit(self, show='Ture'):\n",
    "        self.mynetwork.forward()\n",
    "        _iter = 0\n",
    "        J_history = []\n",
    "        #y_pred = mynetwork.layers[-1].A\n",
    "        while(_iter<1000):\n",
    "            J1 = self._J(Y_train=self.Y_train, network=self.mynetwork)\n",
    "            self.mynetwork.backward()\n",
    "            self.mynetwork.forward()\n",
    "            J2 = self._J(Y_train=self.Y_train, network=self.mynetwork)\n",
    "            #print('4',self.mynetwork.layers[0].W, mynetwork2.layers[0].W)\n",
    "            delta = J1 -J2\n",
    "            \n",
    "            if (delta) < self.epsilon:\n",
    "                break\n",
    "            \n",
    "            _iter += 1\n",
    "            J_history.append(J2)\n",
    "        if show=='Ture':\n",
    "            plt.plot([i for i in range(_iter)],J_history)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test, Y_test, layer_num=None):\n",
    "        if layer_num==None:\n",
    "            layer_num = self.layer_num\n",
    "        self.newnetwork = neuralnetwork(X_train=X_test, Y_train=Y_test, layer_num=layer_num)\n",
    "        for i in range(self.newnetwork.layer_num):\n",
    "            self.newnetwork.layers[i].W = self.mynetwork.layers[i].W.copy()\n",
    "        self.newnetwork.forward()\n",
    "        print(len(self.newnetwork.layers))\n",
    "        y_pred = np.array(self.newnetwork.layers[-1].A[:,:(self.newnetwork.layers[-1].A.shape[1]-1)] > 0.5, dtype='int')\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X_test, Y_test, scoring=metrics.acc_score):\n",
    "        y_pred = self.predict(X_test,Y_test).reshape(-1,).astype('float64')\n",
    "        y_test = Y_test.reshape(-1,)\n",
    "        print(y_pred.shape,y_test)\n",
    "        return scoring(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
